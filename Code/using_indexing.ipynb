{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc348e67",
   "metadata": {},
   "source": [
    "# Effciently answering using INDEX created and presisted\n",
    "ðŸ“Œ This is just a demo of different INDEX usage types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0db02",
   "metadata": {},
   "source": [
    "## Load index from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ddd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "google_genai_api_key = os.getenv(\"GOOGLE_GENAI_API_KEY\")\n",
    "print(google_genai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac0c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# changing the default embedding from OpenAI to GoogleGenAI\n",
    "\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = GoogleGenAIEmbedding(\n",
    "    model_name=\"text-embedding-004\",\n",
    "    embed_batch_size=100,\n",
    "    api_key=google_genai_api_key\n",
    ")\n",
    "\n",
    "# creating GEMINI LLM\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "llm = GoogleGenAI(model=\"gemini-2.0-flash\", api_key=google_genai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9741f761",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dir = './storage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5179761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=index_dir)\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d30b6",
   "metadata": {},
   "source": [
    "## Using Index as :\n",
    "- Retriver\n",
    "- Query Engine\n",
    "- Chat Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ac0e2",
   "metadata": {},
   "source": [
    "### As Retriever\n",
    "- just retrieve the files where we could find the answer for our input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = index.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abced17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver.retrieve(\"Who built ecoride?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecafee16",
   "metadata": {},
   "source": [
    "### As Query Engine\n",
    "- answers the input using the docs we've provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218e4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07445cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\"Who built ecoride?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9601fb6",
   "metadata": {},
   "source": [
    "### As Chat Engine\n",
    "- we chat as with GPT model\n",
    "- it has a memory\n",
    "- has many available modes, we can any one of them.\n",
    "- The available modes are:\n",
    "  - `ChatMode.BEST` (default): Chat engine that uses an agent (react or openai) with a query engine tool\n",
    "  - `ChatMode.CONTEXT`: Chat engine that uses a retriever to get context\n",
    "  - `ChatMode.CONDENSE_QUESTION`: Chat engine that condenses questions\n",
    "  - `ChatMode.CONDENSE_PLUS_CONTEXT`: Chat engine that condenses questions and uses a retriever to get context\n",
    "  - `ChatMode.SIMPLE`: Simple chat engine that uses the LLM directly\n",
    "  - `ChatMode.REACT`: Chat engine that uses a react agent with a query engine tool\n",
    "  - `ChatMode.OPENAI`: Chat engine that uses an openai agent with a query engine tool\n",
    "- default mode is \"best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(llm=llm, chat_mode=\"best\", verbose=True)\n",
    "# verbose = True will print the progress that chat_engine does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea37dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.chat(\"Hi I'm Pavithran G\")\n",
    "chat_engine.chat(\"Who built ecoride\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a80fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing memory of chat_engine\n",
    "chat_engine.chat(\"What's my name?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
